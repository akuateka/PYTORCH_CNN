{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOlx+u8ZvmMAjwaCYP6VmFw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"TyWKcFTECp1H","executionInfo":{"status":"ok","timestamp":1684789855272,"user_tz":-120,"elapsed":7785,"user":{"displayName":"arnaud kuate","userId":"03036251578421473080"}}},"outputs":[],"source":["import torch\n","from torchvision import datasets"]},{"cell_type":"code","source":["# download the FMNIST training dataset\n","fmnist = datasets.FashionMNIST('~/data/FMNIST', download=True, train = True)\n","\n","# fmnist validation dataset\n","val_fmnist = datasets.FashionMNIST('~/data/FMNIST', train = False)"],"metadata":{"id":"AusuXpTuDN7M","executionInfo":{"status":"ok","timestamp":1684789865103,"user_tz":-120,"elapsed":5343,"user":{"displayName":"arnaud kuate","userId":"03036251578421473080"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4e0da218-3dc4-4314-ef3e-d5122add03e0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /root/data/FMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 26421880/26421880 [00:01<00:00, 16196301.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting /root/data/FMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to /root/data/FMNIST/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /root/data/FMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 29515/29515 [00:00<00:00, 268403.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting /root/data/FMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /root/data/FMNIST/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /root/data/FMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4422102/4422102 [00:00<00:00, 5061139.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting /root/data/FMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /root/data/FMNIST/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /root/data/FMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5148/5148 [00:00<00:00, 4369137.39it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting /root/data/FMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /root/data/FMNIST/FashionMNIST/raw\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# training dataset\n","tr_images  = fmnist.data\n","\n","# validation datasets\n","val_images = val_fmnist.data\n","\n","# training labels\n","tr_targets = fmnist.targets\n","\n","# validation labels\n","val_targets = val_fmnist.targets"],"metadata":{"id":"pub-gLl7D6kW","executionInfo":{"status":"ok","timestamp":1684789879409,"user_tz":-120,"elapsed":275,"user":{"displayName":"arnaud kuate","userId":"03036251578421473080"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"metadata":{"id":"0U3S9li8EBhG","executionInfo":{"status":"ok","timestamp":1684790008263,"user_tz":-120,"elapsed":325,"user":{"displayName":"arnaud kuate","userId":"03036251578421473080"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# build a class that fetch the dataset\n","class MyFMNISTDataset(Dataset):\n","    def __init__(self, x, y):\n","        \n","        # convert to float values, flatten\n","        # store them into the device\n","        x = x.float()/255.\n","        x = x.view(-1, 1, 28,28)         #(n_samples, c_channels, height, width)\n","        self.x = x.to(device)\n","        self.y = y.to(device)\n","        \n","    def __getitem__(self, ix):\n","        return self.x[ix], self.y[ix]\n","    \n","    def __len__(self):\n","        return len(self.x)"],"metadata":{"id":"ZbaReK-nEH3x","executionInfo":{"status":"ok","timestamp":1684790039277,"user_tz":-120,"elapsed":329,"user":{"displayName":"arnaud kuate","userId":"03036251578421473080"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# build a function to fetch a batchsize from the dataset\n","def get_data():\n","    # fetch the training dataset and the batches from itself\n","    trn_dts = MyFMNISTDataset(tr_images, tr_targets)\n","    trn_dl = DataLoader(trn_dts, batch_size=32, shuffle=True)\n","    \n","    # fetch the validation dataset and the batches from itself\n","    val_dts = MyFMNISTDataset(val_images, val_targets)\n","    val_dl = DataLoader(val_dts, batch_size=len(val_images), shuffle=False)\n","    return trn_dl, val_dl"],"metadata":{"id":"7vbQjQYJEawH","executionInfo":{"status":"ok","timestamp":1684790054880,"user_tz":-120,"elapsed":202,"user":{"displayName":"arnaud kuate","userId":"03036251578421473080"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["from torch.optim import SGD, Adam\n","\n","# define a model function\n","def get_model():\n","\n","    my_model = nn.Sequential(\n","        nn.Conv2d(1, 64, kernel_size=3),\n","        nn.MaxPool2d(2),\n","        nn.ReLU(),\n","        nn.Conv2d(64, 128, kernel_size=3),\n","        nn.MaxPool2d(2),\n","        nn.ReLU(),\n","        nn.Flatten(),\n","        nn.Linear(3200, 256), \n","        nn.ReLU(),\n","        nn.Linear(256, 10)\n","    ).to(device)\n","    \n","    #  Loss function\n","    loss_fn = nn.CrossEntropyLoss()\n","    \n","    # optimizer\n","    optimizer = Adam(my_model.parameters(), lr = 0.001)\n","    \n","    return my_model, loss_fn, optimizer"],"metadata":{"id":"9bPHC8XrEkJX","executionInfo":{"status":"ok","timestamp":1684790161308,"user_tz":-120,"elapsed":219,"user":{"displayName":"arnaud kuate","userId":"03036251578421473080"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Train the dataset on images batches\n","def train_batch(batch_x, batch_y, my_model, loss_fn, optimizer):\n","    # train mode\n","    my_model.train()\n","    \n","    # flush the previous gradients\n","    optimizer.zero_grad()\n","    \n","    # compute the batch loss\n","    batch_loss = loss_fn(my_model(batch_x), batch_y)\n","    \n","    # backward pass\n","    batch_loss.backward()\n","    \n","    # update parameters\n","    optimizer.step()\n","    \n","    return batch_loss.item()"],"metadata":{"id":"-Jybw28vJPvL","executionInfo":{"status":"ok","timestamp":1684790188617,"user_tz":-120,"elapsed":247,"user":{"displayName":"arnaud kuate","userId":"03036251578421473080"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["@torch.no_grad()\n","def val_loss(val_batch_x, val_batch_y, my_model):\n","    # evaluation mode\n","    my_model.eval()\n","    \n","    # compute validation loss\n","    val_batch_loss = loss_fn(my_model(val_batch_x), val_batch_y)\n","    return val_batch_loss.item()"],"metadata":{"id":"sjJlSp1qN4Xd","executionInfo":{"status":"ok","timestamp":1684790213071,"user_tz":-120,"elapsed":249,"user":{"displayName":"arnaud kuate","userId":"03036251578421473080"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# compute the accuracy\n","\n","# disable gradient computation in the whole function\n","@torch.no_grad()\n","def accuracy(batch_x, batch_y, my_model):\n","    \n","    # evaluation mode\n","    my_model.eval()\n","    \n","    # compute the max and the argmax\n","    max_values, argmaxes = my_model(batch_x).max(-1)\n","    \n","    # coincide with ground truth\n","    is_correct = argmaxes == batch_y\n","    \n","    return is_correct.cpu().numpy().tolist()"],"metadata":{"id":"hMkriuxXOKSI","executionInfo":{"status":"ok","timestamp":1684790227078,"user_tz":-120,"elapsed":247,"user":{"displayName":"arnaud kuate","userId":"03036251578421473080"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["!pip install torch_summary\n","from torchsummary import summary\n","my_model, loss_fn, optimizer = get_model()\n","summary(my_model, torch.zeros(1, 1, 28, 28))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FeizLtsIOSDH","executionInfo":{"status":"ok","timestamp":1684790249861,"user_tz":-120,"elapsed":17499,"user":{"displayName":"arnaud kuate","userId":"03036251578421473080"}},"outputId":"44572616-4651-4ff0-e177-b6bc60461bcb"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch_summary\n","  Downloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\n","Installing collected packages: torch_summary\n","Successfully installed torch_summary-1.4.5\n","==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","├─Conv2d: 1-1                            [-1, 64, 26, 26]          640\n","├─MaxPool2d: 1-2                         [-1, 64, 13, 13]          --\n","├─ReLU: 1-3                              [-1, 64, 13, 13]          --\n","├─Conv2d: 1-4                            [-1, 128, 11, 11]         73,856\n","├─MaxPool2d: 1-5                         [-1, 128, 5, 5]           --\n","├─ReLU: 1-6                              [-1, 128, 5, 5]           --\n","├─Flatten: 1-7                           [-1, 3200]                --\n","├─Linear: 1-8                            [-1, 256]                 819,456\n","├─ReLU: 1-9                              [-1, 256]                 --\n","├─Linear: 1-10                           [-1, 10]                  2,570\n","==========================================================================================\n","Total params: 896,522\n","Trainable params: 896,522\n","Non-trainable params: 0\n","Total mult-adds (M): 10.13\n","==========================================================================================\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.45\n","Params size (MB): 3.42\n","Estimated Total Size (MB): 3.87\n","==========================================================================================\n"]},{"output_type":"execute_result","data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","├─Conv2d: 1-1                            [-1, 64, 26, 26]          640\n","├─MaxPool2d: 1-2                         [-1, 64, 13, 13]          --\n","├─ReLU: 1-3                              [-1, 64, 13, 13]          --\n","├─Conv2d: 1-4                            [-1, 128, 11, 11]         73,856\n","├─MaxPool2d: 1-5                         [-1, 128, 5, 5]           --\n","├─ReLU: 1-6                              [-1, 128, 5, 5]           --\n","├─Flatten: 1-7                           [-1, 3200]                --\n","├─Linear: 1-8                            [-1, 256]                 819,456\n","├─ReLU: 1-9                              [-1, 256]                 --\n","├─Linear: 1-10                           [-1, 10]                  2,570\n","==========================================================================================\n","Total params: 896,522\n","Trainable params: 896,522\n","Non-trainable params: 0\n","Total mult-adds (M): 10.13\n","==========================================================================================\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.45\n","Params size (MB): 3.42\n","Estimated Total Size (MB): 3.87\n","=========================================================================================="]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["trn_dl, val_dl = get_data()"],"metadata":{"id":"3A3E6C4GQBma","executionInfo":{"status":"ok","timestamp":1684790265958,"user_tz":-120,"elapsed":564,"user":{"displayName":"arnaud kuate","userId":"03036251578421473080"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from torch.optim import lr_scheduler\n","scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.8, patience = 0, \n","                                           threshold = 0.001, min_lr = 0.00001,\n","                                           threshold_mode='abs',\n","                                           verbose=True)\n","# empty list of the total losses\n","losses, accuracies = [], []\n","val_losses, val_accuracies = [], []\n","\n","for epoch in range(5):\n","    batch_losses, batch_accuracies = [], []\n","    \n","    for ix, (batch_x, batch_y) in enumerate(iter(trn_dl)):\n","        \n","        batch_loss = train_batch(batch_x, batch_y, my_model, loss_fn, optimizer)\n","        \n","        batch_losses.append(batch_loss)\n","    train_epoch_loss = np.array(batch_losses).mean()\n","    \n","    for ix, batches in enumerate(iter(trn_dl)):\n","        batch_x, batch_y = batches\n","        \n","        batch_accuracy = accuracy(batch_x, batch_y, my_model)\n","        \n","        batch_accuracies.extend(batch_accuracy)\n","    train_epoch_accuracy = np.array(batch_accuracies).mean()\n","    \n","    # Validation set\n","    for ix, (val_batch_x, val_batch_y) in enumerate(iter(val_dl)):\n","        val_batch_accuracies = accuracy(val_batch_x, val_batch_y, my_model)\n","        val_batch_loss = val_loss(val_batch_x, val_batch_y, my_model)\n","        scheduler.step(val_batch_loss)\n","    val_epoch_accuracy = np.array(val_batch_accuracies).mean()\n","        \n","    losses.append(train_epoch_loss)\n","    accuracies.append(train_epoch_accuracy)\n","    val_losses.append(val_batch_loss)\n","    val_accuracies.append(val_epoch_accuracy)\n","    print('Epoch =', str(epoch+1) + ' is done')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jkSyyDPaObP1","executionInfo":{"status":"ok","timestamp":1684790305128,"user_tz":-120,"elapsed":33933,"user":{"displayName":"arnaud kuate","userId":"03036251578421473080"}},"outputId":"5d3b8094-9783-4b9f-f4dc-9586e5d8adfc"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch = 1 is done\n","Epoch = 2 is done\n","Epoch = 3 is done\n","Epoch 00004: reducing learning rate of group 0 to 8.0000e-04.\n","Epoch = 4 is done\n","Epoch 00005: reducing learning rate of group 0 to 6.4000e-04.\n","Epoch = 5 is done\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Z-stgJ19PQi4"},"execution_count":null,"outputs":[]}]}