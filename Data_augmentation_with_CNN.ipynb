{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1501,"status":"ok","timestamp":1685181443027,"user":{"displayName":"arnaud kuate","userId":"03036251578421473080"},"user_tz":-120},"id":"TyWKcFTECp1H"},"outputs":[],"source":["import torch\n","from torchvision import datasets"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1685181443028,"user":{"displayName":"arnaud kuate","userId":"03036251578421473080"},"user_tz":-120},"id":"AusuXpTuDN7M"},"outputs":[],"source":["# download the FMNIST training dataset\n","fmnist = datasets.FashionMNIST('~/data/FMNIST', download=True, train = True)\n","\n","# fmnist validation dataset\n","val_fmnist = datasets.FashionMNIST('~/data/FMNIST', train = False)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1685181443029,"user":{"displayName":"arnaud kuate","userId":"03036251578421473080"},"user_tz":-120},"id":"pub-gLl7D6kW"},"outputs":[],"source":["# training dataset\n","tr_images  = fmnist.data\n","\n","# validation datasets\n","val_images = val_fmnist.data\n","\n","# training labels\n","tr_targets = fmnist.targets\n","\n","# validation labels\n","val_targets = val_fmnist.targets"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1685181443030,"user":{"displayName":"arnaud kuate","userId":"03036251578421473080"},"user_tz":-120},"id":"0U3S9li8EBhG"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":787,"status":"ok","timestamp":1685181443808,"user":{"displayName":"arnaud kuate","userId":"03036251578421473080"},"user_tz":-120},"id":"ZbaReK-nEH3x"},"outputs":[],"source":["# import a module for image transformation\n","from imgaug import augmenters as iaa\n","aug = iaa.Sequential([\n","    iaa.Affine(translate_px={'x':(-10, 10)}, mode = 'constant'), \n","    ])\n","\n","# build a class that fetch the dataset\n","class MyFMNISTDataset(Dataset):\n","    def __init__(self, x, y, aug = None):\n","        \n","        # convert to float values, flatten\n","        # store them into the device\n","        self.x = x.numpy()\n","        self.y = y.numpy()\n","        self.aug = aug\n","        \n","    def __getitem__(self, ix):\n","        return self.x[ix], self.y[ix]\n","    \n","    def __len__(self):\n","        return len(self.x)\n","\n","    # Define collate_fn, which takes the batch of data as input\n","    def collate_fn(self, batch):\n","      \n","      # separate the batch of images and their classes into two\n","      # different variables\n","      ims, classes = list(zip(*batch))\n","\n","      if self.aug:\n","          ims = self.aug.augment_images(images=ims)\n","      \n","      ims = torch.tensor(ims)[:,None,:,:].to(device)/255.\n","      classes = torch.tensor(classes).to(device)\n","      return ims, classes"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1685181443809,"user":{"displayName":"arnaud kuate","userId":"03036251578421473080"},"user_tz":-120},"id":"7vbQjQYJEawH"},"outputs":[],"source":["# build a function to fetch a batchsize from the dataset\n","def get_data():\n","    # fetch the training dataset and the batches from itself\n","    trn_dts = MyFMNISTDataset(tr_images, tr_targets, aug = aug)\n","\n","    # 'notice the collate_fn argument'\n","    # We leveraged the collate_fn method to specify that \n","    # we want to perform augmentations on a batch of images\n","    trn_dl = DataLoader(trn_dts, batch_size=32, collate_fn = trn_dts.collate_fn, shuffle=True)\n","    \n","    # fetch the validation dataset and the batches from itself\n","    val_dts = MyFMNISTDataset(val_images, val_targets)\n","    val_dl = DataLoader(val_dts, batch_size=len(val_images), collate_fn = val_dts.collate_fn, shuffle=False)\n","    return trn_dl, val_dl"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1685181443810,"user":{"displayName":"arnaud kuate","userId":"03036251578421473080"},"user_tz":-120},"id":"9bPHC8XrEkJX"},"outputs":[],"source":["from torch.optim import SGD, Adam\n","\n","# define a model function\n","def get_model():\n","\n","    my_model = nn.Sequential(\n","        nn.Conv2d(1, 64, kernel_size=3),\n","        nn.MaxPool2d(2),\n","        nn.ReLU(),\n","        nn.Conv2d(64, 128, kernel_size=3),\n","        nn.MaxPool2d(2),\n","        nn.ReLU(),\n","        nn.Flatten(),\n","        nn.Linear(3200, 256), \n","        nn.ReLU(),\n","        nn.Linear(256, 10)\n","    ).to(device)\n","    \n","    #  Loss function\n","    loss_fn = nn.CrossEntropyLoss()\n","    \n","    # optimizer\n","    optimizer = Adam(my_model.parameters(), lr = 0.001)\n","    \n","    return my_model, loss_fn, optimizer"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1685181443811,"user":{"displayName":"arnaud kuate","userId":"03036251578421473080"},"user_tz":-120},"id":"-Jybw28vJPvL"},"outputs":[],"source":["# Train the dataset on images batches\n","def train_batch(batch_x, batch_y, my_model, loss_fn, optimizer):\n","    # train mode\n","    my_model.train()\n","    \n","    # flush the previous gradients\n","    optimizer.zero_grad()\n","    \n","    # compute the batch loss\n","    batch_loss = loss_fn(my_model(batch_x), batch_y)\n","    \n","    # backward pass\n","    batch_loss.backward()\n","    \n","    # update parameters\n","    optimizer.step()\n","    \n","    return batch_loss.item()"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1685181443811,"user":{"displayName":"arnaud kuate","userId":"03036251578421473080"},"user_tz":-120},"id":"sjJlSp1qN4Xd"},"outputs":[],"source":["@torch.no_grad()\n","def val_loss(val_batch_x, val_batch_y, my_model):\n","    # evaluation mode\n","    my_model.eval()\n","    \n","    # compute validation loss\n","    val_batch_loss = loss_fn(my_model(val_batch_x), val_batch_y)\n","    return val_batch_loss.item()"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1685181443812,"user":{"displayName":"arnaud kuate","userId":"03036251578421473080"},"user_tz":-120},"id":"hMkriuxXOKSI"},"outputs":[],"source":["# compute the accuracy\n","\n","# disable gradient computation in the whole function\n","@torch.no_grad()\n","def accuracy(batch_x, batch_y, my_model):\n","    \n","    # evaluation mode\n","    my_model.eval()\n","    \n","    # compute the max and the argmax\n","    max_values, argmaxes = my_model(batch_x).max(-1)\n","    \n","    # coincide with ground truth\n","    is_correct = argmaxes == batch_y\n","    \n","    return is_correct.cpu().numpy().tolist()"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FeizLtsIOSDH","outputId":"dde90edc-6b98-4e18-c757-3929bf67b882","executionInfo":{"status":"ok","timestamp":1685181457218,"user_tz":-120,"elapsed":13418,"user":{"displayName":"arnaud kuate","userId":"03036251578421473080"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch_summary in /usr/local/lib/python3.10/dist-packages (1.4.5)\n","==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","├─Conv2d: 1-1                            [-1, 64, 26, 26]          640\n","├─MaxPool2d: 1-2                         [-1, 64, 13, 13]          --\n","├─ReLU: 1-3                              [-1, 64, 13, 13]          --\n","├─Conv2d: 1-4                            [-1, 128, 11, 11]         73,856\n","├─MaxPool2d: 1-5                         [-1, 128, 5, 5]           --\n","├─ReLU: 1-6                              [-1, 128, 5, 5]           --\n","├─Flatten: 1-7                           [-1, 3200]                --\n","├─Linear: 1-8                            [-1, 256]                 819,456\n","├─ReLU: 1-9                              [-1, 256]                 --\n","├─Linear: 1-10                           [-1, 10]                  2,570\n","==========================================================================================\n","Total params: 896,522\n","Trainable params: 896,522\n","Non-trainable params: 0\n","Total mult-adds (M): 10.13\n","==========================================================================================\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.45\n","Params size (MB): 3.42\n","Estimated Total Size (MB): 3.87\n","==========================================================================================\n"]},{"output_type":"execute_result","data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","├─Conv2d: 1-1                            [-1, 64, 26, 26]          640\n","├─MaxPool2d: 1-2                         [-1, 64, 13, 13]          --\n","├─ReLU: 1-3                              [-1, 64, 13, 13]          --\n","├─Conv2d: 1-4                            [-1, 128, 11, 11]         73,856\n","├─MaxPool2d: 1-5                         [-1, 128, 5, 5]           --\n","├─ReLU: 1-6                              [-1, 128, 5, 5]           --\n","├─Flatten: 1-7                           [-1, 3200]                --\n","├─Linear: 1-8                            [-1, 256]                 819,456\n","├─ReLU: 1-9                              [-1, 256]                 --\n","├─Linear: 1-10                           [-1, 10]                  2,570\n","==========================================================================================\n","Total params: 896,522\n","Trainable params: 896,522\n","Non-trainable params: 0\n","Total mult-adds (M): 10.13\n","==========================================================================================\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.45\n","Params size (MB): 3.42\n","Estimated Total Size (MB): 3.87\n","=========================================================================================="]},"metadata":{},"execution_count":11}],"source":["!pip install torch_summary\n","from torchsummary import summary\n","my_model, loss_fn, optimizer = get_model()\n","summary(my_model, torch.zeros(1, 1, 28, 28))"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"3A3E6C4GQBma","executionInfo":{"status":"ok","timestamp":1685181457219,"user_tz":-120,"elapsed":8,"user":{"displayName":"arnaud kuate","userId":"03036251578421473080"}}},"outputs":[],"source":["trn_dl, val_dl = get_data()"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jkSyyDPaObP1","executionInfo":{"status":"ok","timestamp":1685181627541,"user_tz":-120,"elapsed":170329,"user":{"displayName":"arnaud kuate","userId":"03036251578421473080"}},"outputId":"b76cca00-e43c-4e03-9bba-a522356495fe"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-c53fffb7a6f6>:33: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n","  ims = torch.tensor(ims)[:,None,:,:].to(device)/255.\n"]}],"source":["import numpy as np\n","from torch.optim import lr_scheduler\n","scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.8, patience = 0, \n","                                           threshold = 0.001, min_lr = 0.00001,\n","                                           threshold_mode='abs',\n","                                           verbose=True)\n","# empty list of the total losses\n","losses, accuracies = [], []\n","val_losses, val_accuracies = [], []\n","\n","for epoch in range(1):\n","    batch_losses, batch_accuracies = [], []\n","    \n","    for ix, batches in enumerate(iter(trn_dl)):\n","        \n","        batch_x, batch_y = batches\n","        batch_loss = train_batch(batch_x, batch_y, my_model, loss_fn, optimizer)\n","        \n","        batch_losses.append(batch_loss)\n","    train_epoch_loss = np.array(batch_losses).mean()\n","    \n","    for ix, batches in enumerate(iter(trn_dl)):\n","        batch_x, batch_y = batches\n","        \n","        batch_accuracy = accuracy(batch_x, batch_y, my_model)\n","        \n","        batch_accuracies.extend(batch_accuracy)\n","    train_epoch_accuracy = np.array(batch_accuracies).mean()\n","    \n","    # Validation set\n","    for ix, (val_batch_x, val_batch_y) in enumerate(iter(val_dl)):\n","        val_batch_accuracies = accuracy(val_batch_x, val_batch_y, my_model)\n","        val_batch_loss = val_loss(val_batch_x, val_batch_y, my_model)\n","        scheduler.step(val_batch_loss)\n","    val_epoch_accuracy = np.array(val_batch_accuracies).mean()\n","        \n","    losses.append(train_epoch_loss)\n","    accuracies.append(train_epoch_accuracy)\n","    val_losses.append(val_batch_loss)\n","    val_accuracies.append(val_epoch_accuracy)\n","    print('Epoch =', str(epoch+1) + ' is done') "]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNVRLupGJ9FBFuVP5iuK3tL"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}